---
title: "Introduction to blueprintr"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{A Walkthrough of blueprintr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<style>
div.vg-warning {
  background-color:#FFFCE8;
  padding:5px;
}

div.vg-warning span:before {
  content: "‚ö†Ô∏è Warning: ";
}

div.vg-info {
  background-color:#EBFBFF;
  padding:5px;
}

div.vg-info span:before {
  content: "üí° Note: ";
}
</style>

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

set.seed(0x7bae)

cache_location <- tempdir()

drake::clean(cache = drake::drake_cache(cache_location))
```

`blueprintr` is a framework for managing your data assets in a reproducible fashion. While it uses [drake](https://github.com/ropensci/drake) or [targets](https://cran.r-project.org/web/packages/targets/), it adds automated steps for tabular dataset documentation and testing. This allows researchers to create a replicable framework to prevent programming issues from affecting analysis results.

## Installation 

```{r setup, results= FALSE}
# install.packages("remotes")
# remotes::install_github("nyuglobalties/blueprintr")

library(blueprintr)
```

## Designed Use of blueprintr
`blueprintr` provides your data with guardrails typically found in software engineering workflows. 
This allows you to test and document before deploying to production.

The top level of the `blueprintr` workflow is a "blueprints" directory, consisting of `.R` and `.csv` files. 

### About blueprints
Each blueprint has two components to it: 
* Data Construction Spec, usually a `.R` file that instructs drake or targets on how to build a specific dataset. 
* Metadata, usually a `.csv` file that incorporates any mapping files and checks that need to be done on the dataset. 

In order to create a blueprint, we use the `blueprint` function. This function takes three arguments: name (the name of your generated dataset), description (a description of your dataset), command (any functions that need to be applied in order to build the dataset). 

A project may need only a few blueprints, but more likely you'll need nested blueprints to transform the data. 

`blueprintr` generates six "steps" (targets) per blueprint:

Target name             | Description
------------------------|--------------
`{blueprint}_initial`   | The result of running the blueprint's `command`
`{blueprint}_blueprint` | A copy of the blueprint to be used throughout the plan
`{blueprint}_meta`      | A copy of the dataset metadata --- if the metadata file doesn't exist, it will be created in this step
`{blueprint}_meta_path` | Creates the metadata file or loads it
`{blueprint}_checks`    | Runs all tests on the `{blueprint}_initial` target
`{blueprint}`           | The built dataset after running some cleanup tasks

<div class="vg-warning"><span>when writing other targets in your plan, it is advised to **not** refer to the `{blueprint}_initial` step since it could have problems which are discovered in the `{blueprint}_checks` step.</span></div>


## Example

Let's take a well known dataset-- `mtcars`, and create a blueprint for it.

```{r}
# Keeping the row names under the column `rn`
our_mtcars <- mtcars |> tidytable::as_tidytable(rownames = "rn")

# Inspecting our mtcars dataset
head(our_mtcars)
```

We can load in a user-created mapping file. This mapping file acts as a map for any variable name changes, as well as coding changes. 
```{r}
mapping_file <- system.file("mapping/mtcars_item_mapping.csv", package = "blueprintr", mustWork = TRUE)
# Read this csv file:
item_mapping <- mapping_file |>
  readr::read_csv(
    col_types = readr::cols(
      name_1 = readr::col_character(),
      description_1 = readr::col_character(),
      coding_1 = readr::col_character(),
      panel = readr::col_character(),
      homogenized_name = readr::col_character(),
      homogenized_coding = readr::col_character(),
      homogenized_description = readr::col_character()
    )
  )
item_mapping
```

Then, we typically use a tool such as `panelcleaner` to attach our mapping file to the `mtcars` database.
This is a command executed in the dataset construction spec.
```{r}
blueprint(
  "mt_cars",
  description = "mtcars database with attached metadata",
  annotate = TRUE,
  command = {
    pnl <- panelcleaner::enpanel("MTCARS_PANEL", our_mtcars) |>
      panelcleaner::add_mapping(item_mapping) |>
      panelcleaner::homogenize_panel() |>
      panelcleaner::bind_waves() |>
      as.data.frame()

    pnl_name <- get_attr(pnl, "panel_name")
    pnl_mapping <- get_attr(pnl, "mapping")

    pnl <-
      pnl

    class(pnl) <- c("mapped_df", class(pnl))
    set_attrs(pnl, mapping = pnl_mapping, panel_name = pnl_name)
  }
) |>
  bp_include_panelcleaner_meta()
```
When running this code with either `targets` or `drake`, the blueprint metadata is automatically created.
For our mtcars example, this looks like:
```{r, echo= FALSE}
mtcars_metadata <- system.file("project/blueprints/example/homogenized.csv", package = "blueprintr", mustWork = TRUE)
# Read this csv file:
mtcars_metadata |>
  readr::read_csv()
```
Manually editing the metadata allows the user to add tests to check the data type and values. 
And there you have it! You have created your first blueprint on the `mtcars` dataset. 
When running a pipeline with `blueprintr`, the checks allow researchers to be warned of any issues at an early stage, 
allowing them to produce replicable results. 
